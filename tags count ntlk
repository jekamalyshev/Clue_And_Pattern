import pandas as pd
import string
import nltk

from nltk        import WordPunctTokenizer, pos_tag
from nltk.stem   import WordNetLemmatizer
from nltk.corpus import wordnet as wn, stopwords

from nltk        import word_tokenize, pos_tag
from collections import Counter
from itertools   import chain

# Example dataframe
df = pd.DataFrame([[2, "I am new at programming."],
                   [7, "Leaves are falling from the tree."],
                   [4, "Sophia has been studying since this morning."]], columns = ['ID', 'Text'])
                   
df.head(2)

tok_and_tag = lambda x: pos_tag(word_tokenize(x))

df['lower_sent'] = df['Text'].apply(str.lower)
df['tagged_sent'] = df['lower_sent'].apply(tok_and_tag)

possible_tags = sorted(set(list(zip(*chain(*df['tagged_sent'])))[1]))

def add_pos_with_zero_counts(counter, keys_to_add):
    for k in keys_to_add:
        counter[k] = counter.get(k, 0)
    return counter


# Detailed steps.
df['pos_counts'] = df['tagged_sent'].apply(lambda x: Counter(list(zip(*x))[1]))
df['pos_counts_with_zero'] = df['pos_counts'].apply(lambda x: add_pos_with_zero_counts(x, possible_tags))
df['sent_vector'] = df['pos_counts_with_zero'].apply(lambda x: [count for tag, count in sorted(x.most_common())])

# All in one.
df['sent_vector'] = df['tagged_sent'].apply(lambda x:
    [count for tag, count in sorted(
        add_pos_with_zero_counts(
            Counter(list(zip(*x))[1]), 
                    possible_tags).most_common()
         )
    ]
)

df2 = pd.DataFrame(df['sent_vector'].tolist())
df2.columns = possible_tags

df2.head(2)
